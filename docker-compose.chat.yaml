services:
  # Runs on port: 8080
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    environment:
      OLLAMA_BASE_URL: "http://127.0.0.1:11434"
      # For llama-cpp
      OPENAI_API_BASE_URLS: "http://127.0.0.1:8000"
    volumes:
      - ./open-webui:/app/backend/data
    network_mode: host

  ollama:
    image: ollama/ollama
    ports:
      - 11434:11434
    volumes:
      - ./ollama/:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      NVIDIA_VISIBLE_DEVICES: all

  # llama-cpp:
  #   image: ghcr.io/ggml-org/llama.cpp:server-cuda
  #   ports:
  #     - 8000:8080
  #   volumes:
  #     - ./llama.cpp/models/:/models
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   environment:
  #     LLAMA_ARG_PORT: 8080
  #     LLAMA_ARG_HOST: 0.0.0.0

  #     # LLAMA_ARG_CTX_SIZE: 4096
  #     # LLAMA_ARG_N_PREDICT: -1
  #     # LLAMA_ARG_N_PARALLEL: 1

  #     # Alternatively, you can use "LLAMA_ARG_MODEL_URL" to download the model
  #     # LLAMA_ARG_MODEL: /models/2B/model-2b-it.gguf
  #     # LLAMA_ARG_MODEL: /models/4B/model-4b-it.gguf

  #     LLAMA_ARG_N_GPU_LAYERS: 32
  #     NVIDIA_VISIBLE_DEVICES: all
